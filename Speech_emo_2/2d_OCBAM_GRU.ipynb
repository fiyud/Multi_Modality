{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FAUqEGOtatCT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# import dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# import pad sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "avarage = 'micro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2TO5cCVatCX"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, groups=1, reduction=0.0625, kernel_num=4, min_channel=16):\n",
    "        super(Attention, self).__init__()\n",
    "        attention_channel = max(int(in_planes * reduction), min_channel)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_num = kernel_num\n",
    "        self.temperature = 1.0\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(attention_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)\n",
    "        self.func_channel = self.get_channel_attention\n",
    "\n",
    "        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution\n",
    "            self.func_filter = self.skip\n",
    "        else:\n",
    "            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)\n",
    "            self.func_filter = self.get_filter_attention\n",
    "\n",
    "        if kernel_size == 1:  # point-wise convolution\n",
    "            self.func_spatial = self.skip\n",
    "        else:\n",
    "            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)\n",
    "            self.func_spatial = self.get_spatial_attention\n",
    "\n",
    "        if kernel_num == 1:\n",
    "            self.func_kernel = self.skip\n",
    "        else:\n",
    "            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)\n",
    "            self.func_kernel = self.get_kernel_attention\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def skip(_):\n",
    "        return 1.0\n",
    "\n",
    "    def get_channel_attention(self, x):\n",
    "        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return channel_attention\n",
    "\n",
    "    def get_filter_attention(self, x):\n",
    "        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return filter_attention\n",
    "\n",
    "    def get_spatial_attention(self, x):\n",
    "        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)\n",
    "        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)\n",
    "        return spatial_attention\n",
    "\n",
    "    def get_kernel_attention(self, x):\n",
    "        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)\n",
    "        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)\n",
    "        return kernel_attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)\n",
    "\n",
    "\n",
    "class ODConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
    "                 reduction=0.0625, kernel_num=4):\n",
    "        super(ODConv2d, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.kernel_num = kernel_num\n",
    "        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,\n",
    "                                   reduction=reduction, kernel_num=kernel_num)\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if self.kernel_size == 1 and self.kernel_num == 1:\n",
    "            self._forward_impl = self._forward_impl_pw1x\n",
    "        else:\n",
    "            self._forward_impl = self._forward_impl_common\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for i in range(self.kernel_num):\n",
    "            nn.init.kaiming_normal_(self.weight[i], mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.attention.update_temperature(temperature)\n",
    "\n",
    "    def _forward_impl_common(self, x):\n",
    "        # Multiplying channel attention (or filter attention) to weights and feature maps are equivalent,\n",
    "        # while we observe that when using the latter method the models will run faster with less gpu memory cost.\n",
    "        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n",
    "        batch_size, in_planes, height, width = x.size()\n",
    "        x = x * channel_attention\n",
    "        x = x.reshape(1, -1, height, width)\n",
    "        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)\n",
    "        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(\n",
    "            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])\n",
    "        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups * batch_size)\n",
    "        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    def _forward_impl_pw1x(self, x):\n",
    "        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n",
    "        x = x * channel_attention\n",
    "        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups)\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6YcIM0k3atCY"
   },
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = ODConv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = ODConv2d(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pgqN_6wfatCZ"
   },
   "outputs": [],
   "source": [
    "class Dual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dual, self).__init__()\n",
    "\n",
    "\n",
    "        self.feature_extractor2 = nn.Sequential(\n",
    "            # 1st Conv Layer + BatchNorm + ReLU + Pooling\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (128, 251) -> (128, 251, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  # (128, 251, 64) -> (64, 125, 64)\n",
    "\n",
    "            # 2nd Conv Layer + BatchNorm + ReLU + Pooling\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (64, 125, 64) -> (64, 125, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(4, 4), stride=(4, 4)),  # (64, 125, 64) -> (16, 31, 64)\n",
    "\n",
    "            # 3rd Conv Layer + BatchNorm + ReLU + Pooling\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (16, 31, 64) -> (16, 31, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(4, 4), stride=(4, 4)),  # (16, 31, 128) -> (4, 7, 128)\n",
    "\n",
    "            # 4th Conv Layer + BatchNorm + ReLU + Pooling\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (4, 7, 128) -> (4, 7, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(4, 4), stride=(4, 4))  # (4, 7, 128) -> (1, 1, 128)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),  # 1*1*128 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 5),  # 256 -> 5\n",
    "        )\n",
    "\n",
    "        # self.truefc = nn.Softmax(dim=1)\n",
    "        self.gru = nn.GRU(input_size=128, hidden_size=256, bidirectional=False, batch_first=True)\n",
    "        self.fc3 = nn.Linear(5, 5)\n",
    "        self.cbam = CBAM(128)\n",
    "\n",
    "    def forward(self, mfcc):\n",
    "        # wave_form = self.feature_extractor1(wave_form)  # Pass through the sequential feature extractor\n",
    "\n",
    "        # # LSTM expects input of shape (batch_size, seq_len, input_size)\n",
    "        # wave_form = wave_form.permute(0, 2, 1)  # Reshape to (batch_size, seq_len, input_size)\n",
    "        # wave_form, _ = self.lstm(wave_form)\n",
    "\n",
    "        # wave_form = wave_form[:, -1, :]\n",
    "\n",
    "        # wave_form = self.fc1(wave_form)\n",
    "\n",
    "        mfcc = self.feature_extractor2(mfcc)\n",
    "        mfcc = self.cbam(mfcc)\n",
    "      \n",
    "        if mfcc.shape[0] != 1:\n",
    "            mfcc = mfcc.squeeze().unsqueeze(1)\n",
    "    \n",
    "        elif mfcc.shape[0] == 1:\n",
    "            mfcc = mfcc.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "       \n",
    "        mfcc,_ = self.gru(mfcc)\n",
    "        mfcc = self.fc2(mfcc)\n",
    "        # x = torch.cat((wave_form, mfcc), dim=1)\n",
    "\n",
    "        mfcc = self.fc3(mfcc)\n",
    "\n",
    "        return mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxCFN8YRatCa"
   },
   "outputs": [],
   "source": [
    "n_mfcc = 128\n",
    "window_size = 2048\n",
    "strides = 512\n",
    "window_size_stft = 1024\n",
    "window = np.hanning(window_size_stft)\n",
    "\n",
    "def load_emodata(link, sr = 16000, duration = 5):\n",
    "    # Load the audio file\n",
    "    wave_form, _ = librosa.load(path=link, sr=sr)\n",
    "    # label = father folder name\n",
    "    labels = os.path.basename(os.path.dirname(link))\n",
    "\n",
    "    if len(wave_form) < sr * duration:\n",
    "        # zero pad the audio if its less than 5 seconds\n",
    "        wave_form = np.pad(wave_form, (0, sr * duration - len(wave_form)), 'symmetric')\n",
    "        mfcc1 = librosa.feature.mfcc(y=wave_form, sr=8000, n_mfcc=n_mfcc, n_fft=window_size, hop_length=strides)\n",
    "        # stft1 = librosa.core.spectrum.stft(wave_form, n_fft=window_size_stft, hop_length=256, window=window)\n",
    "        # spect1 = 2 * np.abs(stft1) / np.sum(window)\n",
    "        return np.array([mfcc1]), np.array([labels])\n",
    "\n",
    "    elif len(wave_form) == sr * duration:\n",
    "        mfcc2 = librosa.feature.mfcc(y=wave_form, sr=8000, n_mfcc=n_mfcc, n_fft=window_size, hop_length=strides)\n",
    "        # stft2 = librosa.core.spectrum.stft(wave_form, n_fft=window_size_stft, hop_length=256, window=window)\n",
    "        # spect2 = 2 * np.abs(stft2) / np.sum(window)\n",
    "        # return the audio as it is\n",
    "        return np.array([mfcc2]), np.array([labels])\n",
    "\n",
    "\n",
    "    else:\n",
    "        wave_segments = []\n",
    "        _labels = [labels] * (len(wave_form) // (sr * duration) + 1)\n",
    "        for i in range(0, len(wave_form), sr * duration):\n",
    "            wave_segments.append(wave_form[i:i + sr * duration])\n",
    "\n",
    "        # If the last segment is less than 5 seconds, then pad it with the last 5 seconds of the second last segment\n",
    "        len_wave_segments_last = len(wave_segments[-1])\n",
    "        padding = sr * duration - len_wave_segments_last\n",
    "        temp = np.append(wave_segments[-2][sr * duration - padding:], wave_segments[-1])\n",
    "        wave_segments[-1] = temp\n",
    "\n",
    "        mfcc_seg = librosa.feature.mfcc(y=np.array(wave_segments), sr=8000, n_mfcc=n_mfcc, n_fft=window_size, hop_length=strides)\n",
    "        # stft_seg = librosa.core.spectrum.stft(wave_segments, n_fft=window_size_stft, hop_length=256, window=window)\n",
    "        # spect_seg = 2 * np.abs(stft_seg) / np.sum(window)\n",
    "\n",
    "        return np.array(mfcc_seg) , np.array(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1724873769739,
     "user": {
      "displayName": "Nguyễn Đức Quang Anh",
      "userId": "05768917261476937201"
     },
     "user_tz": -420
    },
    "id": "U-O3eW7watCd",
    "outputId": "bf6f8503-0fb8-4917-d6ef-3dc8c8d599f2"
   },
   "outputs": [],
   "source": [
    "# EMO_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, sr=16000, duration=5, n_mfcc=128, n_fft=2048, hop_length=512):\n",
    "    # Load audio file\n",
    "    wave_form, _ = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # Pad or trim the audio to match expected duration\n",
    "    if len(wave_form) < sr * duration:\n",
    "        wave_form = np.pad(wave_form, (0, sr * duration - len(wave_form)), mode='symmetric')\n",
    "    else:\n",
    "        wave_form = wave_form[:sr * duration]\n",
    "    \n",
    "    # Compute MFCC features\n",
    "    mfcc_features = librosa.feature.mfcc(y=wave_form, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    return np.array([mfcc_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4-_TyZc8bQhR"
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model, file_path):\n",
    "    # Preprocess audio file\n",
    "    mfcc_features = preprocess_audio(file_path)\n",
    "    mfcc_features = torch.tensor(mfcc_features).unsqueeze(0)\n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(mfcc_features)\n",
    "        \n",
    "    # covert to percentage\n",
    "    return (F.softmax(output.squeeze(), dim=0) * 100).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r\"C:\\Users\\Admin\\Desktop\\Desktop\\aidemo\\aidemo\\Speech_emo_2\\models\\emopre.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_emotion = predict_emotion(model, r\"C:\\Users\\Admin\\Documents\\Zalo Received Files\\emodata\\emodata\\b\\recordings (37)_clip_3.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99.99734497070312,\n",
       " 0.000279890198726207,\n",
       " 2.50759892850283e-08,\n",
       " 0.0023407244589179754,\n",
       " 5.255967698758468e-05]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_emotion"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dfine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
